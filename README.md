# Context-aware-Interactive-Attention-for-Multi-modal-Sentiment-and Emotion-Analysis 
Context-aware-Interactive-Attention-for-Multi-modal-Sentiment-and Emotion-Analysis (https://www.aclweb.org/anthology/D19-1566/).

For the evaluation of our proposed CIA approach, we employ five multi-modal benchmark datasets. These datasets can be accessed through (https://github.com/A2Zadeh/CMU-MultimodalSDK) or You can download datasets from this link (https://drive.google.com/drive/folders/1IVgdjfRGSqnai45ksot7UZ5C-1xJBBWZ?usp=sharing).

five multi-modal benchmark datasets i.e., YouTube, MOUD, ICT-MMMO, CMU-MOSI, and CMU-MOSEI.

first download the dataset from given link and set the path in the code accordingly.
make two folder (i) results and (ii) weights

how to run file:

For YouTube dataset:

for trimodal-->> python trimodal_YouTube.py

========================

--versions--

python: 2.7

keras: 2.2.2

tensorflow: 1.9.0

## Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis 
Code for the paper [Context-aware-Interactive-Attention-for-Multi-modal-Sentiment-and Emotion-Analysis](https://www.aclweb.org/anthology/D19-1566/) (EMNLP 2019).

For the evaluation of our proposed CIA approach, we employ five multi-modal benchmark datasets i.e., YouTube, MOUD, ICT-MMMO, CMU-MOSI, and CMU-MOSEI.

### Dataset

* You can access these datasets from [here](https://github.com/A2Zadeh/CMU-MultimodalSDK) or 

* You can download datasets from [here](https://drive.google.com/drive/folders/1IVgdjfRGSqnai45ksot7UZ5C-1xJBBWZ?usp=sharing).

* Download the dataset from given link and set the path in the code accordingly and make two folder (i) results and (ii) weights

### How to Run:

### For YouTube dataset: 

For trimodal -->> python trimodal_YouTube.py

========================

### --versions--

python: 2.7

keras: 2.2.2

tensorflow: 1.9.0

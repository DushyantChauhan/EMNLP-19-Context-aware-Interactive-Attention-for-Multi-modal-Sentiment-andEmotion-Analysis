# Context-aware-Interactive-Attention-for-Multi-modal-Sentiment-and Emotion-Analysis 
(https://www.iitp.ac.in/~ai-nlp-ml/papers/emnlp19-emotion.pdf)

For the evaluation of our proposed CIA approach, we employ five multi-modal benchmark datasets. These datasets can be accessed through (https://github.com/A2Zadeh/CMU-MultimodalSDK).

five multi-modal benchmark datasets i.e., YouTube, MOUD, ICT-MMMO, CMU-MOSI, and CMU-MOSEI.

first download the dataset from given link and set the path in the code accordingly.
make two folder (i) results and (ii) weights

how to run file:

python trimodal_YouTube.py

python version 2.7
